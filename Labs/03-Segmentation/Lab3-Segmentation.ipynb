{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision, Lab 3: Segmentation\n",
    "\n",
    "Today we'll experiment with methods to segment a scene. Since we've been working with a sample ground robot in an indoor environment, we'll take as an example the task of segmenting unoccupied ground plane space from obstacles.\n",
    "\n",
    "If we can successfully determine which pixels in the image seen by our robot are on the flat floor and which are likely obstacles, we can combine that information with the ground plane homography-based rectification method we developed in the last lab to obtain a map of the space around the robot.\n",
    "\n",
    "The same approach could be used by an autonomous vehicle to determine where, in the image from its front-facing camera, the road is and where possible obstacles are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Color-based segmentation\n",
    "\n",
    "In our indoor example, we have fairly consistent lighting and a consistently-colored floor, so a color based approach may work.\n",
    "\n",
    "The task here is, given an input pixel at location $(x,y)$ with RGB color $(r,g,b)$, classify the pixel as \"floor\" or \"not floor.\"\n",
    "\n",
    "This is a classic machine learning problem. We have an input feature vector $(r, g, b)$ (some methods would also utilize $x$ and $y$ as well), and a desired output of 1\n",
    "for floor and 0 for non-floor.\n",
    "\n",
    "We probably want to transform the color space from RGB to HSV, since the RGB vector mixes color information with intensity information in the same measurements:\n",
    "\n",
    "<img src=\"img/lab03-1.jpg\" width=\"300\"/>\n",
    "\n",
    "whereas the HSV color space separates color from saturation (\"color purity\") and intensity:\n",
    "\n",
    "<img src=\"img/lab03-2.jpg\" width=\"300\"/>\n",
    "\n",
    "(Images are from Wikimedia Commons via the OpenCV documentation.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could use a fancy classifier like logistic regression or a SVM, this problem is easily solved with a generative model. We apply the principle of maximum a posteriori classification and Bayes' rule:\n",
    "\n",
    "$$\n",
    "f(h, s, v) = \n",
    "    \\begin{cases}\n",
    "        1 & p(\\text{floor} \\mid h, s, v) > 0.5 \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\text{floor} \\mid h, s, v) = \\frac{p(h, s, v \\mid \\text{floor}) p(\\text{floor})}{p(h, s, v)}\n",
    "$$\n",
    "\n",
    "and since $p(h, s, v)$ is not known, we eliminate it:\n",
    "\n",
    "$$\n",
    "f(h, s, v) =\n",
    "    \\begin{cases}\n",
    "        1 & p(h, s, v \\mid \\text{floor}) p(\\text{floor}) > p(h, s, v | \\text{not floor}) p(\\text{not floor}) \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "The entity $p(\\text{floor)$ is easy to estimate as the proportion of pixels in a sample of images that are floor pixels.\n",
    "\n",
    "What about $p(h, s, v | floor)$?\n",
    "\n",
    "The simplest approach here is just a lookup table. Since we're conditioning on floor, we just need to sample a bunch of floor pixels and record the frequency of each value of $h$, $s$, and $v$.\n",
    "\n",
    "The problem with that approach is that we'd need something like $10 * 255 * 255 * 255$ pixels (168M!) to get a decent estimate of the value in every bin of this frequency table.\n",
    "\n",
    "What we do instead is collapse some values of $h$, $s$, and $v$ into bins. For example, we might drop the 4 least significant bits of each of these values, then we'd end up needing only about $10 * 16 * 16 * 16$ (just 41K) samples to get a decent estimate.\n",
    "\n",
    "Note that some methods would further drop the V element, which represents intensity, on the assumption that the global amount of lighting doesn't matter.\n",
    "\n",
    "In fact, as the sun moves across the sky, the color composition of the light changes, and artificial light is quite different in color than sunlight. Still, it's a start.\n",
    "\n",
    "So we want to collapse the $(h, s, v)$ vector into two features, $(h>>4, s>>4)$. We'll end up with a lookup table containing just $16*16 = 256$ elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save an image-plane-to-ground-plane homography to a YML file\n",
    "\n",
    "Let's use a simple version of the program from lab 2 that calls `getPerspectiveTransform()`\n",
    "to get a ground plane to image plane homography and save it to a YML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <opencv2/opencv.hpp>\n",
    "\n",
    "using namespace cv;\n",
    "using namespace std;\n",
    "\n",
    "#define VIDEO_FILE \"robot.mp4\"\n",
    "#define HOMOGRAPHY_FILE \"robot-homography.yml\"\n",
    "\n",
    "Mat matPauseScreen, matResult, matFinal;\n",
    "Point point;\n",
    "vector<Point> pts;\n",
    "int var = 0;\n",
    "int drag = 0;\n",
    "\n",
    "// Create mouse handler function\n",
    "void mouseHandler(int event, int x, int y, int, void*)\n",
    "{\n",
    "    if (var >= 4) return;\n",
    "    if (event == EVENT_LBUTTONDOWN) // Left button down\n",
    "    {\n",
    "        drag = 1;\n",
    "        matResult = matFinal.clone();\n",
    "        point = Point(x, y);\n",
    "        if (var >= 1) \n",
    "        {\n",
    "            line(matResult, pts[var - 1], point, Scalar(0, 255, 0, 255), 2);\n",
    "        }\n",
    "        circle(matResult, point, 2, Scalar(0, 255, 0), -1, 8, 0);\n",
    "        imshow(\"Source\", matResult);\n",
    "    }\n",
    "    if (event == EVENT_LBUTTONUP && drag) // When Press mouse left up\n",
    "    {\n",
    "        drag = 0; var++;\n",
    "        pts.push_back(point);\n",
    "        matFinal = matResult.clone();\n",
    "        if (var >= 4)\n",
    "        {\n",
    "            line(matFinal, pts[0], pts[3], Scalar(0, 255, 0, 255), 2);\n",
    "            fillPoly(matFinal, pts, Scalar(0, 120, 0, 20), 8, 0);\n",
    "\n",
    "            setMouseCallback(\"Source\", NULL, NULL);\n",
    "        }\n",
    "        imshow(\"Source\", matFinal);\n",
    "    }\n",
    "    if (drag)\n",
    "    {\n",
    "        matResult = matFinal.clone();\n",
    "        point = Point(x, y);\n",
    "        if (var >= 1) \n",
    "            line(matResult, pts[var - 1], point, Scalar(0, 255, 0, 255), 2);\n",
    "        circle(matResult, point, 2, Scalar(0, 255, 0), -1, 8, 0);\n",
    "        imshow(\"Source\", matResult);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    Mat matFrameCapture;\n",
    "    Mat matFrameDisplay;\n",
    "    int key = -1;\n",
    "\n",
    "    VideoCapture videoCapture(VIDEO_FILE);\n",
    "    if (!videoCapture.isOpened()) {\n",
    "        cerr << \"ERROR! Unable to open input video file \" << VIDEO_FILE << endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    while (key < 0)        // play video until press any key\n",
    "    {\n",
    "        // Get the next frame\n",
    "        videoCapture.read(matFrameCapture);\n",
    "        if (matFrameCapture.empty()) {\n",
    "            // End of video file\n",
    "            break;\n",
    "        }\n",
    "        float ratio = 640.0 / matFrameCapture.cols;\n",
    "        resize(matFrameCapture, matFrameDisplay, cv::Size(), ratio, ratio, INTER_LINEAR);\n",
    "\n",
    "        imshow(VIDEO_FILE, matFrameDisplay);\n",
    "        key = waitKey(30);\n",
    "\n",
    "        if (key >= 0)\n",
    "        {\n",
    "            destroyWindow(VIDEO_FILE);\n",
    "            matPauseScreen = matFrameCapture;\n",
    "            matFinal = matPauseScreen.clone();\n",
    "\n",
    "            namedWindow(\"Source\", WINDOW_AUTOSIZE);\n",
    "            setMouseCallback(\"Source\", mouseHandler, NULL);\n",
    "            imshow(\"Source\", matPauseScreen);\n",
    "            waitKey(0);\n",
    "            destroyWindow(\"Source\");\n",
    "\n",
    "            Point2f src[4];\n",
    "            for (int i = 0; i < 4; i++)\n",
    "            {\n",
    "                src[i].x = pts[i].x * 1.0;\n",
    "                src[i].y = pts[i].y * 1.0;\n",
    "            }\n",
    "            Point2f reals[4];\n",
    "            reals[0] = Point2f(800.0, 800.0);\n",
    "            reals[1] = Point2f(1000.0, 800.0);\n",
    "            reals[2] = Point2f(1000.0, 1000.0);\n",
    "            reals[3] = Point2f(800.0, 1000.0);\n",
    "\n",
    "            Mat homography_matrix = getPerspectiveTransform(src, reals);\n",
    "            std::cout << \"Estimated Homography Matrix is:\" << std::endl;\n",
    "            std::cout << homography_matrix << std::endl;\n",
    "\n",
    "            // perspective transform operation using transform matrix\n",
    "            cv::warpPerspective(matPauseScreen, matResult, homography_matrix, matPauseScreen.size(), cv::INTER_LINEAR);\n",
    "            imshow(\"Source\", matPauseScreen);\n",
    "            imshow(\"Result\", matResult);\n",
    "\n",
    "            waitKey(0);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "VIDEO_FILE = \"robot.mp4\"\n",
    "HOMOGRAPHY_FILE = \"robot-homography.yml\"\n",
    "\n",
    "matResult = None\n",
    "matFinal = None\n",
    "matPauseScreen = None\n",
    "\n",
    "point = (-1, -1)\n",
    "pts = []\n",
    "var = 0 \n",
    "drag = 0\n",
    "\n",
    "# Mouse handler function has 5 parameters input (no matter what)\n",
    "def mouseHandler(event, x, y, flags, param):\n",
    "    global point, pts, var, drag, matFinal, matResult\n",
    "\n",
    "    if (var >= 4):                           # if homography points are more than 4 points, do nothing\n",
    "        return\n",
    "    if (event == cv2.EVENT_LBUTTONDOWN):     # When Press mouse left down\n",
    "        drag = 1                             # Set it that the mouse is in pressing down mode\n",
    "        matResult = matFinal.copy()          # copy final image to draw image\n",
    "        point = (x, y)                       # memorize current mouse position to point var\n",
    "        if (var >= 1):                       # if the point has been added more than 1 points, draw a line\n",
    "            cv2.line(matResult, pts[var - 1], point, (0, 255, 0, 255), 2)    # draw a green line with thickness 2\n",
    "        cv2.circle(matResult, point, 2, (0, 255, 0), -1, 8, 0)             # draw a current green point\n",
    "        cv2.imshow(\"Source\", matResult)      # show the current drawing\n",
    "    if (event == cv2.EVENT_LBUTTONUP and drag):  # When Press mouse left up\n",
    "        drag = 0                             # no more mouse drag\n",
    "        pts.append(point)                    # add the current point to pts\n",
    "        var += 1                             # increase point number\n",
    "        matFinal = matResult.copy()          # copy the current drawing image to final image\n",
    "        if (var >= 4):                                                      # if the homograpy points are done\n",
    "            cv2.line(matFinal, pts[0], pts[3], (0, 255, 0, 255), 2)   # draw the last line\n",
    "            cv2.fillConvexPoly(matFinal, np.array(pts, 'int32'), (0, 120, 0, 20))        # draw polygon from points\n",
    "        cv2.imshow(\"Source\", matFinal);\n",
    "    if (drag):                                    # if the mouse is dragging\n",
    "        matResult = matFinal.copy()               # copy final images to draw image\n",
    "        point = (x, y)                            # memorize current mouse position to point var\n",
    "        if (var >= 1):                            # if the point has been added more than 1 points, draw a line\n",
    "            cv2.line(matResult, pts[var - 1], point, (0, 255, 0, 255), 2)    # draw a green line with thickness 2\n",
    "        cv2.circle(matResult, point, 2, (0, 255, 0), -1, 8, 0)         # draw a current green point\n",
    "        cv2.imshow(\"Source\", matResult)           # show the current drawing\n",
    "\n",
    "def main():\n",
    "    global matFinal, matResult, matPauseScreen\n",
    "    key = -1;\n",
    "\n",
    "    videoCapture = cv2.VideoCapture(VIDEO_FILE)\n",
    "    if not videoCapture.isOpened():\n",
    "        print(\"ERROR! Unable to open input video file \", VIDEO_FILE)\n",
    "        return -1\n",
    "\n",
    "    width  = videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)   # float `width`\n",
    "    height = videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float `height`\n",
    "\n",
    "    # Capture loop \n",
    "    while (key < 0):\n",
    "        # Get the next frame\n",
    "        _, matFrameCapture = videoCapture.read()\n",
    "        if matFrameCapture is None:\n",
    "            # End of video file\n",
    "            break\n",
    "\n",
    "        ratio = 640.0 / width\n",
    "        dim = (int(width * ratio), int(height * ratio))\n",
    "        matFrameDisplay = cv2.resize(matFrameDisplay, dim)\n",
    "\n",
    "        cv2.imshow(VIDEO_FILE, matFrameDisplay)\n",
    "        key = cv2.waitKey(30)\n",
    "\n",
    "        if (key >= 0):\n",
    "            cv2.destroyWindow(VIDEO_FILE)\n",
    "            matPauseScreen = matFrameCapture\n",
    "            matFinal = matPauseScreen.copy()\n",
    "            cv2.namedWindow(\"Source\", cv2.WINDOW_AUTOSIZE)\n",
    "            cv2.setMouseCallback(\"Source\", mouseHandler)\n",
    "            cv2.imshow(\"Source\", matPauseScreen)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyWindow(\"Source\")\n",
    "\n",
    "            if (len(pts) < 4):\n",
    "                return\n",
    "\n",
    "            src = np.array(pts).astype(np.float32)\n",
    "            reals = np.array([(800, 800),\n",
    "                                (1000, 800),\n",
    "                                (1000, 1000),\n",
    "                                (800, 1000)], np.float32)\n",
    "\n",
    "            homography_matrix = cv2.getPerspectiveTransform(src, reals);\n",
    "            print(\"Estimated Homography Matrix is:\")\n",
    "            print(homography_matrix)\n",
    "\n",
    "            h, w, ch = matPauseScreen.shape\n",
    "            matResult = cv2.warpPerspective(matPauseScreen, homography_matrix, (w, h), cv2.INTER_LINEAR)\n",
    "            matPauseScreen = cv2.resize(matPauseScreen, dim)\n",
    "            cv2.imshow(\"Source\", matPauseScreen)\n",
    "            matResult = cv2.resize(matResult, dim)\n",
    "            cv2.imshow(\"Result\", matResult)\n",
    "\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class for reading/writing homography data\n",
    "\n",
    "Let's make a structure for our homography data. Objects\n",
    "of class <code>HomographyData</code> will keep the information about a homography. It contains:\n",
    "\n",
    "- cPoints: number of points for setting homography\n",
    "- aPoints: points information\n",
    "- matH: Homography matrix\n",
    "- widthOut: image width of the output image\n",
    "- heightOut: image height of the output image\n",
    "\n",
    "We need two methods: reading and writing homographies.\n",
    "\n",
    "At this step, you should create a new cpp file for create the special functions and structure. Please create 2 files: <code>HomographyData.h</code> and <code>HomographyData.cpp</code>\n",
    "\n",
    "For Visual Studio users: <code>HomographyData.h</code> should be created in your\n",
    "\"Header Files\" section, and <code>HomographyData.cpp</code> should be created in\n",
    "your \"Source Files\" section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++ header\n",
    "\n",
    "Place the following in `HomographyData.h`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <opencv2/opencv.hpp>\n",
    "\n",
    "class HomographyData\n",
    "{\n",
    "public:\n",
    "    cv::Mat matH;\n",
    "    int widthOut;\n",
    "    int heightOut;\n",
    "    int cPoints;\n",
    "    cv::Point2f aPoints[4];\n",
    "\n",
    "    HomographyData();\n",
    "    HomographyData(std::string homography_file);\n",
    "\n",
    "    bool read(std::string homography_file);\n",
    "    bool write(std::string homography_file);\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++ source file\n",
    "\n",
    "Then provide the implementation in `HomographyData.cpp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include \"homography.h\"\n",
    "\n",
    "Homography::Homography(std::string homography_file)\n",
    "{\n",
    "    read(homography_file);\n",
    "}\n",
    "\n",
    "HomographyData::HomographyData()\n",
    "{\n",
    "    cPoints = 0;\n",
    "}\n",
    "\n",
    "bool HomographyData::read(std::string homography_file)\n",
    "{\n",
    "    cv::FileStorage fileStorage(homography_file, cv::FileStorage::Mode::READ);\n",
    "    if (!fileStorage.isOpened()) {\n",
    "        return false;\n",
    "    }\n",
    "    cv::FileNode points = fileStorage[\"aPoints\"];\n",
    "    cv::FileNodeIterator it = points.begin(), it_end = points.end();\n",
    "    cPoints = 0;\n",
    "    for (int i = 0; it != it_end; it++, i++) {\n",
    "        (*it) >> aPoints[i];\n",
    "        cPoints++;\n",
    "    }\n",
    "    fileStorage[\"matH\"] >> matH;\n",
    "    fileStorage[\"widthOut\"] >> widthOut;\n",
    "    fileStorage[\"heightOut\"] >> heightOut;\n",
    "    fileStorage.release();\n",
    "    return true;\n",
    "}\n",
    "\n",
    "bool HomographyData::write(std::string homography_file)\n",
    "{\n",
    "    cv::FileStorage fileStorage(homography_file, cv::FileStorage::Mode::WRITE);\n",
    "    if (!fileStorage.isOpened()) {\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "    fileStorage << \"aPoints\" << \"[\";\n",
    "    for (int i = 0; i < 4; i++)\n",
    "    {\n",
    "        fileStorage << aPoints[i];\n",
    "    }\n",
    "    fileStorage << \"]\";\n",
    "    fileStorage << \"matH\" << matH;\n",
    "    fileStorage << \"widthOut\" << widthOut;\n",
    "    fileStorage << \"heightOut\" << heightOut;\n",
    "    fileStorage.release();\n",
    "    return true;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List #use it for :List[...]\n",
    "\n",
    "class Homograpy:\n",
    "    matH = np.zeros((3, 3))\n",
    "    widthOut : int\n",
    "    heightOut : int\n",
    "    cPoints : int\n",
    "    aPoints:list = []\n",
    "\n",
    "    def __init__(self, homography_file = None):\n",
    "        self.cPoints = 0\n",
    "        if homography_file is not None:\n",
    "            self.read(homography_file)\n",
    "\n",
    "    def read(self, homography_file):\n",
    "        fileStorage = cv2.FileStorage(homography_file, cv2.FILE_STORAGE_READ)\n",
    "        if not fileStorage.isOpened():\n",
    "            return False\n",
    "\n",
    "        self.cPoints = 0\n",
    "        for i in range(points.size()):\n",
    "            points = fileStorage.getNode(\"aPoints\" + str(i))\n",
    "            self.aPoints.append(points.mat())\n",
    "            self.cPoints += 1\n",
    "        self.matH = fileStorage.getNode(\"matH\").mat()\n",
    "        self.widthOut = int(fileStorage.getNode(\"widthOut\").real())\n",
    "        self.heightOut = int(fileStorage.getNode(\"heightOut\").real())\n",
    "        fileStorage.release()\n",
    "        return True\n",
    "\n",
    "    def write(self, homography_file):\n",
    "        fileStorage = cv2.FileStorage(homography_file, cv2.FILE_STORAGE_WRITE)\n",
    "        if not fileStorage.isOpened():\n",
    "            return False\n",
    "\n",
    "        for i in range(4):\n",
    "            fileStorage.write(\"aPoints\" + str(i), self.aPoints[i])\n",
    "\n",
    "        fileStorage.write(\"matH\", self.matH)\n",
    "        fileStorage.write(\"widthOut\", self.widthOut)\n",
    "        fileStorage.write(\"heightOut\", self.heightOut)\n",
    "        fileStorage.release()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using it in C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include \"HomographyData.h\"\n",
    "\n",
    "...\n",
    "\n",
    "// Write H to file\n",
    "\n",
    "HomographyData homographyData;\n",
    "for (int i = 0; i < 4; i++)\n",
    "{\n",
    "    homographyData.aPoints[i] = src[i];\n",
    "    homographyData.cPoints++;\n",
    "}\n",
    "homographyData.matH = homography_matrix;\n",
    "homographyData.widthOut = matPauseScreen.cols;\n",
    "homographyData.heightOut = matPauseScreen.rows;\n",
    "if (!homographyData.write(HOMOGRAPHY_FILE)) {\n",
    "    cerr << \"ERROR! Unable to write homography data file \" << HOMOGRAPHY_FILE << endl;\n",
    "    return -1;\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "// Read H from file\n",
    "\n",
    "HomographyData homographyData(HOMOGRAPHY_FILE);\n",
    "if (!homographyData.cPoints == 0) {\n",
    "    cerr << \"ERROR! Unable to read homography data file \" << HOMOGRAPHY_FILE << endl;\n",
    "    return -1;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using it in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write H to file\n",
    "\n",
    "homographyData = Homography()\n",
    "homographyData.cPoints = 0\n",
    "for i in range(4):\n",
    "    homographyData.aPoints.append(src[i])\n",
    "    homographyData.cPoints += 1\n",
    "homographyData.matH = homography_matrix\n",
    "homographyData.widthOut = width\n",
    "homographyData.heightOut = height\n",
    "homographyData.write(HOMOGRAPHY_FILE)\n",
    "\n",
    "...\n",
    "\n",
    "# Read H from file\n",
    "\n",
    "homographyData = Homography(HOMOGRAPHY_FILE)\n",
    "if homographyData.cPoints == 0:\n",
    "    print(\"ERROR! Unable to read homography data file \", HOMOGRAPHY_FILE)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The output file `robot-homography.yml` might contain the following:\n",
    "\n",
    "    %YAML:1.0\n",
    "    ---\n",
    "    aPoints0: !!opencv-matrix\n",
    "       rows: 2\n",
    "       cols: 1\n",
    "       dt: d\n",
    "       data: [ 860., 49. ]\n",
    "    aPoints1: !!opencv-matrix\n",
    "       rows: 2\n",
    "       cols: 1\n",
    "       dt: d\n",
    "       data: [ 1386., 52. ]\n",
    "    aPoints2: !!opencv-matrix\n",
    "       rows: 2\n",
    "       cols: 1\n",
    "       dt: d\n",
    "       data: [ 1620., 375. ]\n",
    "    aPoints3: !!opencv-matrix\n",
    "       rows: 2\n",
    "       cols: 1\n",
    "       dt: d\n",
    "       data: [ 784., 367. ]\n",
    "    matH: !!opencv-matrix\n",
    "       rows: 3\n",
    "       cols: 3\n",
    "       dt: d\n",
    "       data: [ -1.1391186748382573e-03, -2.2794919900117339e-03,\n",
    "           1.0627648167793186e-16, 1.6604445559011895e-05,\n",
    "           -3.2917310423991194e-03, -3.1863931027743840e-02,\n",
    "           -1.6241141452077307e-09, -1.8250980274059071e-06,\n",
    "           -9.0126408686546570e-04 ]\n",
    "    widthOut: 2419\n",
    "    heightOut: 1250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a pre-computed homography in a new program\n",
    "\n",
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <opencv2/opencv.hpp>\n",
    "#include <iostream>\n",
    "#include \"homography.h\"\n",
    "\n",
    "using namespace cv;\n",
    "using namespace std;\n",
    "\n",
    "#define VIDEO_FILE \"robot.mp4\"\n",
    "#define HOMOGRAPHY_FILE \"robot-homography.yml\"\n",
    "\n",
    "void displayFrame(cv::Mat& matFrameDisplay, int iFrame, int cFrames, tsHomographyData* pHomographyData) {\n",
    "    for (int i = 0; i < pHomographyData->cPoints; i++) {\n",
    "        cv::circle(matFrameDisplay, pHomographyData->aPoints[i], 10, cv::Scalar(255, 0, 0), 2, cv::LINE_8, 0);\n",
    "    }\n",
    "    imshow(VIDEO_FILE, matFrameDisplay);\n",
    "    stringstream ss;\n",
    "    ss << \"Frame \" << iFrame << \"/\" << cFrames;\n",
    "    ss << \": hit <space> for next frame or 'q' to quit\";\n",
    "    //cv::displayOverlay(VIDEO_FILE, ss.str(), 0);  // for linux + qt\n",
    "    putText(matFrameDisplay, ss.str(), Point(10, 30), FONT_HERSHEY_SIMPLEX, 1.0, Scalar(0, 0, 255), 3);\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cv::Mat matFrameCapture;\n",
    "    cv::Mat matFrameDisplay;\n",
    "    int cFrames;\n",
    "    tsHomographyData homographyData;\n",
    "\n",
    "    cv::VideoCapture videoCapture(VIDEO_FILE);\n",
    "    if (!videoCapture.isOpened()) {\n",
    "        cerr << \"ERROR! Unable to open input video file \" << VIDEO_FILE << endl;\n",
    "        return -1;\n",
    "    }\n",
    "    cFrames = (int)videoCapture.get(cv::CAP_PROP_FRAME_COUNT);\n",
    "\n",
    "    // Create a named window that will be used later to display each frame\n",
    "    cv::namedWindow(VIDEO_FILE, (unsigned int)cv::WINDOW_NORMAL | cv::WINDOW_KEEPRATIO | cv::WINDOW_GUI_EXPANDED);\n",
    "\n",
    "    // Read homography from file\n",
    "    if (!readHomography(HOMOGRAPHY_FILE, &homographyData)) {\n",
    "        cerr << \"ERROR! Unable to read homography data file \" << HOMOGRAPHY_FILE << endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    int iFrame = 0;\n",
    "    while (true) {\n",
    "\n",
    "        // Block for next frame\n",
    "\n",
    "        videoCapture.read(matFrameCapture);\n",
    "        if (matFrameCapture.empty()) {\n",
    "            // End of video file\n",
    "            break;\n",
    "        }\n",
    "\n",
    "        displayFrame(matFrameCapture, iFrame, cFrames, &homographyData);\n",
    "\n",
    "        int iKey;\n",
    "        do {\n",
    "            iKey = cv::waitKey(10);\n",
    "            if (getWindowProperty(VIDEO_FILE, cv::WND_PROP_VISIBLE) == 0) {\n",
    "                return 0;\n",
    "            }\n",
    "            if (iKey == (int)'q' || iKey == (int)'Q') {\n",
    "                return 0;\n",
    "            }\n",
    "        } while (iKey != (int)' ');\n",
    "        iFrame++;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from homography import tsHomographyData, readHomography, writeHomography\n",
    "\n",
    "VIDEO_FILE = \"robot.mp4\"\n",
    "HOMOGRAPHY_FILE = \"robot-homography.yml\"\n",
    "\n",
    "def displayFrame(matFrameDisplay, iFrame, cFrames, pHomographyData):\n",
    "    for i in range(pHomographyData.cPoints):\n",
    "        cv2.circle(matFrameDisplay, pHomographyData.aPoints[i], 10, (255, 0, 0), 2, cv.LINE_8, 0)\n",
    "\n",
    "    cv2.imshow(VIDEO_FILE, matFrameDisplay)\n",
    "    ss = \"Frame \" + str(iFrame) + \"/\" + str(cFrames)\n",
    "    ss += \": hit <space> for next frame or 'q' to quit\";\n",
    "    # cv2.displayOverlay(VIDEO_FILE, ss, 0);  # for linux + qt\n",
    "    cv2.putText(matFrameDisplay, ss, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3);\n",
    "\n",
    "def main():\n",
    "    global matFinal, matResult, matPauseScreen\n",
    "    key = -1;\n",
    "\n",
    "    videoCapture = cv2.VideoCapture(VIDEO_FILE)\n",
    "    if not videoCapture.isOpened():\n",
    "        print(\"ERROR! Unable to open input video file \", VIDEO_FILE)\n",
    "        return -1\n",
    "\n",
    "    width  = videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)   # float `width`\n",
    "    height = videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float `height`\n",
    "\n",
    "    cFrames = videoCapture.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    cv2.namedWindow(VIDEO_FILE, cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO | cv2.WINDOW_GUI_EXPANDED)\n",
    "\n",
    "    homographyData = readHomography(HOMOGRAPHY_FILE)\n",
    "    if homographyData is None:\n",
    "        print(\"ERROR! Unable to read homography data file \", HOMOGRAPHY_FILE)\n",
    "        return -1\n",
    "\n",
    "    iFrame = 0\n",
    "    # Capture loop \n",
    "    while (key < 0):\n",
    "        # Get the next frame\n",
    "        _, matFrameCapture = videoCapture.read()\n",
    "        if matFrameCapture is None:\n",
    "            # End of video file\n",
    "            break\n",
    "\n",
    "        matFrameCapture = videoCapture.read(matFrameCapture)\n",
    "        if matFrameCapture is None:\n",
    "            # End of video file\n",
    "            break\n",
    "\n",
    "        displayFrame(matFrameCapture, iFrame, cFrames, homographyData)\n",
    "\n",
    "        iKey = -1\n",
    "        while iKey != ord(' '):\n",
    "            iKey = cv2.waitKey(10)\n",
    "            if (getWindowProperty(VIDEO_FILE, cv2.WND_PROP_VISIBLE) == 0):\n",
    "                return 0\n",
    "            if (iKey == ord('q') or iKey == ord('Q')):\n",
    "                return 0\n",
    "\n",
    "        iFrame += 1\n",
    "\n",
    "        return\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick version of segmentation\n",
    "\n",
    "As a simple experiment, find a pixel in the image that is clearly on the floor, convert the input image to the HSV colorspace using code\n",
    "\n",
    "    cvtColor(matFrameBGR, matFrameHSV, COLOR_BGR2HSV);\n",
    "\n",
    "If you display HSV as an image you might get something like this:\n",
    "\n",
    "<img src=\"img/lab03-4.png\" width=\"600\"/>\n",
    "\n",
    "Output the HSV values at your preferred location. Create a 2D array <code>double aProbFloorHS[16][16]</code> and set the selected bin to a probability of 1.0 and all others to 0.\n",
    "Show, in a second window, a mask for the pixels selected by $p(h, s, v \\mid \\text{floor}) > 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain HS probabilities using a machine learning model\n",
    "\n",
    "Next, take the first frame from the video, load it in <link>[gimp](https://www.gimp.org/downloads/)</link> or other image editing software, and make a mask for the floor pixels.\n",
    "\n",
    "To extract frames from the video at a frame rate of one frame per second of the video, try at the command line\n",
    "\n",
    "    $ ffmpeg -i robot.mp4 -r 1 -f image2 frame-%03d.png\n",
    "\n",
    "Or you can save the images in your OpenCV program:\n",
    "\n",
    "    stringstream ss;\n",
    "    ss << fixed << setprecision(3) << setfill('0');     // set fix digit prefix length to 3 and the blank digit is filled by 0\n",
    "    ss << \"frame-\" << setw(3) << iFrame << \".png\";      // set iFrame length to 3 from command above\n",
    "    imwrite(ss.str(), matFrameCapture);\n",
    "\n",
    "To make a mask in gimp, add a 50% transparent layer, color pixels you want to select, and delete the original image layer with: \n",
    "\n",
    "(right click $\\rightarrow$ Layer $\\rightarrow$ Stack $\\rightarrow$ Select Next Layer), and then\n",
    "\n",
    "(right click $\\rightarrow$ Layer $\\rightarrow$ Delete Layer).\n",
    "\n",
    "<img src=\"img/lab03-5.png\" width=\"300\"/>\n",
    "\n",
    "<img src=\"img/lab03-6.png\" width=\"600\"/>\n",
    "\n",
    "<img src=\"img/lab03-7.png\" width=\"600\"/>\n",
    "\n",
    "You can copy the resulting layer then create a new image from the clipboard and export as a black and white PNG.\n",
    "\n",
    "<img src=\"img/lab03-8.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add code to your program to read the mask, apply it to the first frame of the video, and calculate the entries in the <code>aProbFloorHS</code> array. You'll also need a <code>aProbNonFloorHS</code> array and total pixel counts.\n",
    "\n",
    "Note: The code is just an example, it might have error or incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double aProbFloorHS[16][16];\n",
    "double aProbNonFloorHS[16][16];\n",
    "void readProbfloor(string mask_file)\n",
    "{\n",
    "    Mat mask = imread(mask_file);\n",
    "    cvtColor(mask, mask, COLOR_BGR2GRAY);\n",
    "    int scaleH = matFrameHSV.rows / 16;\n",
    "    int scaleW = matFrameHSV.cols / 16;\n",
    "    double area = scaleH * scaleW * 1.0;\n",
    "    for (int ax = 0; ax < 16; ax++)\n",
    "    {\n",
    "        for (int ay = 0; ay < 16; ay++)\n",
    "        {\n",
    "            int floor = 0;\n",
    "            for (int x = 0; x < scaleW; x++)\n",
    "            {\n",
    "                for (int y = 0; y < scaleH; y++)\n",
    "                {\n",
    "                    int pix = mask.at<int>(ay * scaleH + y, ax * scaleW + x);\n",
    "                    if (pix > 128)\n",
    "                        floor++;\n",
    "                }\n",
    "            }\n",
    "            aProbFloorHS[ax][ay] = floor / area;\n",
    "            aProbNonFloorHS[ax][ay] = 1 - aProbFloorHS[ax][ay];\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aProbFloorHS = np.zeros((16,16))\n",
    "aProbNonFloorHS = np.zeros((16,16))\n",
    "\n",
    "void readProbfloor(mask_file)\n",
    "{\n",
    "    mask = cv2.imread(mask_file)\n",
    "    cv2.cvtColor(mask, mask, cv2.COLOR_BGR2GRAY)\n",
    "    scaleH = matFrameHSV.rows // 16\n",
    "    scaleW = matFrameHSV.cols // 16\n",
    "    area = scaleH * scaleW * 1.0\n",
    "    for ax in range(16):\n",
    "        for ay in range(16):\n",
    "            floor = 0\n",
    "            for x in range(scaleW):\n",
    "                for y in range(scaleH):\n",
    "                    pix = mask[ay * scaleH + y, ax * scaleW + x]\n",
    "                    if (pix > 128):\n",
    "                        floor += 1\n",
    "            aProbFloorHS[ax,ay] = floor / area;\n",
    "            aProbNonFloorHS[ax,ay] = 1 - aProbFloorHS[ax,ay]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to get a good probabilistic floor color model yourself\n",
    "\n",
    "Finally, display the mask in a partially transparent color on top of the image as the video is rendered. How well does it work? You might want to add additional images to your training set. Consider a tool such as hasty.ai to mark up multiple images.\n",
    "\n",
    "With a training set of 19 images, 64 bins for H, 16 bins for S, and 16 bins for V, I got a leave-one-out cross validated test accuracy of around 95%, with an F1 for the floor of 0.98 and an F1 for the obstacles of 0.51. See how well your model works and include this information in your report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "\n",
    "We've seen that color-based segmentation in HSV space using a generative machine learning model can be effective but has limitations when the objects to be segmented from the background have color distributions similar to the background. The best accuracy we could get for our floor/obstacle model last week was around 95%, but hat was with occasional errors grouping in large enough regions that would preclude our robot from navigating safely in the indoor environment.\n",
    "\n",
    "Semantic segmentation attempts to address this issue using a more sophisticated model to separate the scene into its constituent regions more effectively.\n",
    "\n",
    "Typical semantic segmentation models typically use a lot of resources. For example, the state of the art models published for the MIT ADE20K dataset in their <link>[GitHub repository](https://github.com/CSAILVision/semantic-segmentation-pytorch)</link> are very accurate and run at 2-17 FPS on a NVIDIA Pascal Titan Xp GPU. They also run well on a GTX 1080TI. But on an i7 CPU, I found that they take 9-11 SECONDS PER FRAME, and on an NVIDIA Jetson Nano's GPU, they take 12-42 SEDONDS PER FRAME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lighter semantic segmentation models\n",
    "\n",
    "We would like to experiment with some semantic segmenation models that have a hope of running in real time on a small embedded system such as the Jetson Nano.\n",
    "\n",
    "NVIDIA has published a very useful repository <link>[Jetson Inference](https://github.com/dusty-nv/jetson-inference)</link> that contains versions of two semantic segmentation models: SegNet and UNet.\n",
    "\n",
    "I ran the SegNet model in this repository using FCN-ResNet18 trained on Pascal VOC with 320x320 input images. It takes a while to load the model into memory and so on, but inference time once all is ready is very fast: less than 70 ms.\n",
    "\n",
    "So it's fast! Unfortunately, it doesn't work particularly well out of the box:\n",
    "\n",
    "<img src=\"img/lab03-3.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we shouldn't expect it to, considering that the Pascal VOC dataset \"only\" contains 20 classes plus the \"background\" (the black label), and others of which are \"bottle\" (the purple label) and \"person\" (the tan label).\n",
    "\n",
    "That's on the NVIDIA Jetson Nano. The model itself was built with PyTorch, but it has been exported in ONNX (Open Neural Network Exchange) format, and the Jetson Nano executes it on TensorRT. You can download the <link>[ONNX model from AIT](https://www.cs.ait.ac.th/~mdailey/class/vision/fcn_resnet18.onnx)</link> (I just copied it from the excellent Jetson Inference repository).\n",
    "\n",
    "If you'd like to understand the structure of the model represented in this ONNX file, download the <link>[Netron 4.3.4 AppImage for Linux](https://github.com/lutzroeder/netron)</link>, run the AppImage, and load the file. You'll see that it\n",
    "\n",
    "1. Takes as input a 320x320 3-channel image\n",
    "2. Performs 64 7x7 convolutions\n",
    "3. Does batch normalization, ReLU, and MaxPool\n",
    "4. Runs a residual block with the following\n",
    "    1. 64 3x3 convolutions then BN then ReLU\n",
    "    2. 64 3x3 convolutions then BN\n",
    "5. ReLU\n",
    "6. Residual block (same structure as above)\n",
    "7. ReLU\n",
    "8. Residual block with the following\n",
    "    1. 128 3x3 convolutions then BN then ReLU\n",
    "    2. 128 3x3 convolutions then BN\n",
    "    3. Residual add using 128 1x1 convs to expand feature map\n",
    "9. Etc. (several more residual blocks and downscaling by a factor of 32)\n",
    "10. Final 21 1x1 convolution to obtain output 1x21x10x10 tensor\n",
    "\n",
    "Since this model is relatively small and at least generates some output, let's see if we can get it running on our OpenCV video stream. A model that runs nice and fast on an Intel CPU and on a Jetson GPU, in C++/OpenCV as well as Python, would be perfect.\n",
    "\n",
    "<img src=\"img/lab03-10.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN ResNet 18\n",
    "\n",
    "Let's see if we can get this ONNX model running in OpenCV.\n",
    "\n",
    "First, let's initialize with the Pascal VOC classes and read an image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string aStringClasses[] = {\n",
    "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "};\n",
    "\n",
    "cv::Vec3b aColorClasses[] = {\n",
    "        { 0, 0, 0 }, { 255, 0, 0 }, { 0, 255, 0 }, { 0, 255, 120 }, { 0, 0, 255 }, { 255, 0, 255 }, { 70, 70, 70 },\n",
    "        { 102, 102, 156 }, { 190, 153, 153 }, { 180, 165, 180 }, { 150, 100, 100 }, { 153, 153, 153 },\n",
    "        { 250, 170, 30 }, { 220, 220, 0 }, { 107, 142, 35 }, { 192, 128, 128 }, { 70, 130, 180 }, { 220, 20, 60 },\n",
    "        { 0, 0, 142 }, { 0, 0, 70 }, { 119, 11, 32 }\n",
    "};\n",
    "\n",
    "int nClasses = sizeof(aColorClasses) / 3;\n",
    "\n",
    "// Read CNN definition\n",
    "\n",
    "auto net = cv::dnn::readNetFromONNX(ONNX_NETWORK_DEFINITION);\n",
    "\n",
    "// Read input image\n",
    "\n",
    "cv::Mat matFrame = cv::imread(IMAGE_FILE);\n",
    "if (matFrame.empty()) {\n",
    "    cerr << \"Cannot open image file \" << IMAGE_FILE << endl;\n",
    "    return -1;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aStringClasses = [\n",
    "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "\n",
    "aColorClasses = [\n",
    "        ( 0, 0, 0 ), ( 255, 0, 0 ), ( 0, 255, 0 ), ( 0, 255, 120 ), ( 0, 0, 255 ), ( 255, 0, 255 ), ( 70, 70, 70 ),\n",
    "        ( 102, 102, 156 ), ( 190, 153, 153 ), ( 180, 165, 180 ), ( 150, 100, 100 ), ( 153, 153, 153 ),\n",
    "        ( 250, 170, 30 ), ( 220, 220, 0 ), ( 107, 142, 35 ), ( 192, 128, 128 ), ( 70, 130, 180 ), ( 220, 20, 60 ),\n",
    "        ( 0, 0, 142 ), ( 0, 0, 70 ), ( 119, 11, 32 )\n",
    "]\n",
    "\n",
    "nClasses = len(aColorClasses)\n",
    "\n",
    "# Read CNN definition\n",
    "net = cv2.dnn.readNetFromONNX(cv2.ONNX_NETWORK_DEFINITION)\n",
    "\n",
    "# Read input image\n",
    "matFrame = cv2.imread(IMAGE_FILE);\n",
    "if (matFrame is None):\n",
    "    print(\"Cannot open image file \", IMAGE_FILE)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the input image is preprocessed to form a tensor suitable for input to our DNN model, we can just set the input layer of the network to point to the newly preprocessed image tensor, then we can do a forward pass through the network model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Propagate the matInputTensor through the FCN model\n",
    "\n",
    "net.setInput(matInputTensor);\n",
    "cv::Mat matScore = net.forward();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagate the matInputTensor through the FCN model\n",
    "net.setInput(matInputTensor)\n",
    "matScore = net.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK but what about the preprocessing?\n",
    "\n",
    "Here's the thing: most CNN models (even object detection and image segmentation models) are based on a classification model as the \"backbone\" of the model. In the case of FCN-ResNet-18, the backbone is of course ResNet-18.\n",
    "\n",
    "Usually, training a model based on a classifier begins by loading weights trained for classification on ImageNet or another dataset then further training and/or \"fine tuning\" the model on a more specific dataset. We do this because we don't want to spend the week of GPU time it takes to get a model that analyzes image edges, puts them together into higher-level shapes, and gradually extracts a set of coarsely localized features that describe objects of interest.\n",
    "\n",
    "When data scientists train a model on ImageNet, they almost always do a few common steps:\n",
    "\n",
    "1. Scale the input image's R, G, and B intensities (normally in the range 0-255) to the range 0-1.\n",
    "2. Scale the input image to the size needed for the classifier, or sample a patch from the input the size required by the classifier.\n",
    "3. Subtract expected mean values for the R, G, and B channels. The magic values for ImageNet are 0.485 for R, 0.456 for G, and 0.406 for B.\n",
    "4. Divide by expected standard deviations for the R, G, and B channels. The magic values for ImageNet are 0.229 for R, 0.224 for G, and 0.225 for B.\n",
    "\n",
    "There is an OpenCV function <code>cv::dnn::blobFromImage()</code> that performs some of these steps but not all. Check the documentation and add the necessary code to prepare the image for presentation for the pre-trained network.\n",
    "\n",
    "Next, you'll want to use the result of the semantic segmentation model to color the input image and display for examination by the user, and perhaps add some information about CPU time required for the inference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Colorize the image and display\n",
    "\n",
    "cv::Mat matColored;\n",
    "colorizeSegmentation(matFrame, matScore, matColored, aColorClasses, aStringClasses, nClasses);\n",
    "\n",
    "// Add timing information\n",
    "\n",
    "std::vector<double> layersTimes;\n",
    "double freq = cv::getTickFrequency() / 1000;\n",
    "double t = net.getPerfProfile(layersTimes) / freq;\n",
    "std::string label = cv::format(\"Inference time: %.2f ms\", t);\n",
    "cv::putText(matColored, label, cv::Point(10,30),\n",
    "        cv::FONT_HERSHEY_SIMPLEX, 1.0, cv::Scalar(0, 255, 0));\n",
    "\n",
    "// Display\n",
    "\n",
    "cv::namedWindow(WINDOW_NAME, WINDOW_FLAGS);\n",
    "cv::imshow(WINDOW_NAME, matColored);\n",
    "cv::waitKey(0);\n",
    "\n",
    "return 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colorize the image and display\n",
    "\n",
    "matColored = cv2.colorizeSegmentation(matFrame, matScore, aColorClasses, aStringClasses, nClasses)\n",
    "\n",
    "# Add timing information\n",
    "layersTimes = 0\n",
    "freq = cv2.getTickFrequency() / 1000;\n",
    "t = net.getPerfProfile(layersTimes) / freq;\n",
    "label = \"Inference time: \" + str(t) + \" ms\"\n",
    "cv2.putText(matColored, label, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0))\n",
    "\n",
    "# Display\n",
    "cv2.namedWindow(WINDOW_NAME, WINDOW_FLAGS)\n",
    "cv2.imshow(WINDOW_NAME, matColored)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll have to figure out colorizeSegmentation for yourself. See if you can get a result similar to the image above. It won't be exactly the same, as the jetson inference code scales the input image slightly differently from <code>blobFromImage()</code>. I got the following:\n",
    "\n",
    "<img src=\"img/lab03-11.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning on our own dataset\n",
    "\n",
    "Next, we will want to fine-tune the FCN-ResNet-18 model to our own floor/obstacle dataset. We will load the existing weights, throw away the 21-class output layer of the existing model, and replace it with our own two-class output layer. We'll keep the weights for all but this last layer, then start training on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that we could run a pre-trained FCN-ResNet-18 on our robot floor/obstacle video sequence using a model exported to ONNX format and executed using OpenCV's DNN module.\n",
    "\n",
    "However, the stock model, which was trained on VOC 2012's 21 classes, was unable to give a recognizable segmentation on our data set.\n",
    "\n",
    "Therefore, we learn how to train a semantic segmentation model on a benchmark dataset (VOC 2012) then fine tune that model to our dataset, with the hope that the result will be effective.\n",
    "\n",
    "Also, since we will be training on a medium-sized dataset (VOC), we should set up our machine learning model development environment to use a powerful GPU server rather than our poor little laptops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU server setup\n",
    "\n",
    "This lab should work fine on the AIT DS&AI JupyterHub\n",
    "server <link>[login here](https://puffer.cs.ait.ac.th/hub/login?next=)</link>\n",
    "\n",
    "You might instead want to create a custom environment according to\n",
    "<link>[RTML GPU setup](https://github.com/dsai-asia/RTML/blob/main/Labs/01-Setup/01-Setup.ipynb)</link>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Scripts\n",
    "\n",
    "Download the <link>[training scripts and data for this lab](https://drive.google.com/file/d/1ihaFWQLTsFPpzAfAHfhrt1cR4WEwhH7r/view)</link>\n",
    "\n",
    "Move the code and data into your project directory and take a look at train.py and the code it uses. The code is originally from torchvision but modified by Dustin Franklin at NVIDIA for some smaller models that will run on the Jetson Nano, especially FCN-ResNet-18.\n",
    "\n",
    "### Train on Pascal VOC\n",
    "\n",
    "Use train.py to learn an initial model from the Pascal VOC 2012 (21 class) dataset. You'll want to train for about 100 epochs and take the model with the best IoU on the validation set. Expect about 52% IoU.\n",
    "\n",
    "If you configured the runtime environment as above, the model weights after each iteration as well as the best model will be saved to /workspace in the container, which is mapped to $HOME/workspace on the host (puffer in our case).\n",
    "\n",
    "Transfer learning: Fine tune on our robot floor dataset\n",
    "Take a look at <code>retrain.py</code>. This script loads the pre-trained FCN-ResNet-18 we built in the previous step and fine tunes it on the robot floor data set. Take a look at the code and play around with it, only fine tuning the fresh output layer or also tuning the layers already trained on VOC. You'll also want to experiment with the relative weighting of floor vs. obstacle classes in the loss function. If you don't give obstacles (the rare class) a high weight, the model will learn to classify everything as floor!\n",
    "\n",
    "I didn't put the code to save the model weights here. To get your model saved, read and understand the two scripts train.py and fine_tune.py and add the code to save your model to the fine tuning script.\n",
    "\n",
    "Give some thought to what criteria you should use for the \"best model so far.\" If you only cared about per-pixel accuracy, you would probably just classify all pixels as \"floor.\" What you probably want to maximize is the mean of the IoU for the two classes.\n",
    "\n",
    "### Export to ONNX and run under DNN\n",
    "\n",
    "Once that's all working, export the model to ONNX using the provided script and see if you can get it running on OpenCV DNN.\n",
    "\n",
    "### Put it all together!\n",
    "\n",
    "Finally, you should be able to display, frame by frame, the input image with floor/non-floor pixels identified and a birds-eye view map of the obstacle-free space around\n",
    "the robot (using the homography you saved earlier).\n",
    "\n",
    "Post a video of your result to Piazza before the next lab, and turn in a report describing your experiments and results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
